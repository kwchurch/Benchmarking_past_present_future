# Benchmarking_past_present_future
<h1>Workshop Home Page for Benchmarking: Past, Present and Future</h1>

Where have we been, and where are we going?  It is easier to talk about the past than the future. These days,
benchmarks evolve more bottom up (such as papers with code). There used to
be more top-down leadership from government (and industry, in the  case of
systems, with benchmarks such as <a href="https://www.spec.org/benchmarks.html">SPEC</a>). Going forward, there may be more
top-down leadership from organizations like <a href="https://mlperf.org/"> MLPerf </a> and/or influencers like
<a href="https://en.wikipedia.org/wiki/David_Ferrucci"> David Ferrucci </a>, who was responsible for IBM’s success with <a href="https://www.youtube.com/watch?v=P18EdAKuC1U">Jeopardy</a>, and
has recently written a paper suggesting how the community should think
about benchmarking for machine comprehension (<a href="https://arxiv.org/pdf/2005.01525.pdf">To Test Machine Comprehension, Start by Defining Comprehension</a>). Tasks such as reading
comprehension become even more interesting as we move beyond English.
Multilinguality introduces many challenges, and even more opportunities.

We have an amazing collection of invited speakers that can share with us first hand knowledge of how benchmarking became important in Information Retrieval, and then in speech (starting around 1975), and then in language (in 1988).  Much of this history is described in this video and two 2016 Interspeech keynotes: Makhoul describes how benchmarking overcame resistance in speech in <a href="https://www.superlectures.com/interspeech2016/isca-medalist-for-leadership-and-extensive-contributions-to-speech-and-language-processing">this keynote</a>, and Jurafsky describes how this approach moved from speech to language in <a href="https://www.superlectures.com/interspeech2016/ketchup-interdisciplinarity-and-the-spread-of-innovation-in-speech-and-language-processing">this keynote</a>.

<!--
It is easier to talk about the past than the future.  These days, benchmarks evolve more bottom up (such as papers with code).  There used to be more top-down leadership from government (and industry, in the case of systems, with benchmarks such as <a href="https://www.spec.org/benchmarks.html">SPEC</a>).  Going forward, there may be more top-down leadership from organizations like <a href="https://mlperf.org/"> MLPerf </a> and/or influencers like <a href="https://en.wikipedia.org/wiki/David_Ferrucci"> David Ferrucci </a>, who was responsible for IBM’s success with <a href="https://www.youtube.com/watch?v=P18EdAKuC1U">Jeopardy</a>, and has recently written a paper suggesting how the community should think about benchmarking for machine comprehension (<a href="https://arxiv.org/pdf/2005.01525.pdf">To Test Machine Comprehension, Start by Defining Comprehension</a>).  Tasks such as reading comprehension become even more interesting as we move beyond English.  Multilinguality introduces many challenges, and even more opportunities. -->

<h2>Important Dates</h2>

<ul>
  <li> April 26, 2021: Paper submission </li>
  <li>  May 28, 2021: Notification of acceptance </li>
  <li>  June 7, 2021: Camera-ready papers due </li>
  <li>  August 5-6, 2021: Workshop dates </li>
  </ul>

<h2>Invited Speakers</h2> 

We have an amazing collection of invited talks, many with direct first-hand knowledge of this history, and many insights for the future.

<ol> <li>Past
  <ol>
<li>John Makhoul </li>
<li>Mark Liberman</li>
<li>Ellen Voorhees </li>
 </ol> </li>
<li>Present
  <ol>
<li>Ming Zhou </li>
<li>Hua Wu and Jing Liu </li>
<li>Neville Ryant </li>
<li>Brian MacWhinney and Saturnino Haider</li>
<li>Samuel Bowman </li>
<li>Douwe Kiela </li>
<li>Eunsol Choi </li>
<li>Anders Søgaard</li>
   </ol> </li>
<li> Future
  <ol>
<li>Greg Diamos  </li>
<li>David Ferrucci </li>
<li>Ido Dagan </li>
  </ol> </li>
</ol>

<h2>Submissions</h2>

We accept three types of submissions, long papers, short papers and
abstracts, all following the ACL2021 style, and the <a href="https://www.aclweb.org/adminwiki/index.php?title=ACL_Policies_for_Submission,_Review_and_Citation">ACL submission policy</a>:


Long papers may consist of up to eight (8) pages of content, plus
unlimited references, short papers may consist of up to four (4) pages of
content; final versions will be given one additional page of content so
that reviewers' comments can be taken into account. Abstracts may consist
of up to two (2) pages of content, plus unlimited references but will not
be given any additional page upon acceptance. Submissions should be sent
in electronic forms, using the Softconf START conference management
system. The submission site will be announced on the workshop page once
available.

We invite original research papers from a wide range of topics, including
but not limited to:

<ol>
<li>What important technologies and underlying sciences need to be fostered, now and in the future?</li>
<li>In each case, are there existing tasks/benchmarks that move the field in the right direction?</li>
<li>Where are there gaps?</li>
<li>For the gaps, are there initial steps that are accessible, attractive, and cost effective?</li>
<li>How large should a benchmark be?  
  <ol>
<li>How much data do we need to measure significant differences?  </li>
<li>How much data do machines need to obtain good performance?  </li>
<li>How much data do babies need to learn language? </li>
  </ol> </li>
</ol>

</ul>
<h2>Program Committee</h2>

 
<ol>
<li> Nicoletta Calzolari </li>
<li> Kenneth Church </li>
<li> Valia Kordoni </li>
<li> Julia Hirshberg </li>
<li> Lori Lamel </li>
<li> Mark Liberman</li>
  </ol>

Contact us <a href = "mailto: pc-benchmarking-ws-acl2021@googlegroups.com">here</a>, if you have any questions.

